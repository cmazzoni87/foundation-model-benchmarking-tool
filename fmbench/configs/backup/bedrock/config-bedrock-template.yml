general:
  name: bedrock-benchmark-template

aws:
  region: {region}
  bucket: {write_bucket}
  s3_and_or_local_file_system: s3

datasets:
  filters:
    - language: en
      min_length_in_tokens: 0
      max_length_in_tokens: 500
      payload_file: payload-{lang}-{min}-{max}.jsonl
    - language: en
      min_length_in_tokens: 501
      max_length_in_tokens: 1000
      payload_file: payload-{lang}-{min}-{max}.jsonl
    - language: en
      min_length_in_tokens: 1001
      max_length_in_tokens: 1500
      payload_file: payload-{lang}-{min}-{max}.jsonl
  prompt_template_keys:
    - input
    - context
  ds_N: 50
  ground_truth_col_key: ground_truth

s3_read_data:
  source_data_files:
    - hf:nayonika/convfinqa
  read_bucket: {read_bucket}
  prompt_template_dir: prompt_template
  prompt_template_file: prompt_template_claude.txt
  s3_or_local_file_system: s3
  source_data_prefix: source_data
  scripts_prefix: scripts
  tokenizer_prefix: tokenizer

dir_paths:
  all_prompts_file: all_prompts.csv
  data_prefix: data
  prompts_prefix: prompts
  metadata_dir: metadata

run_steps:
  0_setup.ipynb: true
  1_generate_data.ipynb: true
  2_deploy_model.ipynb: true
  3_run_inference.ipynb: true
  4_get_evaluations.ipynb: false
  5_model_metric_analysis.ipynb: true
  6_cleanup.ipynb: false

inference_parameters:
  claude-eval:
    temperature: 0.0
    max_tokens: 4096
    top_p: 0.9
  bedrock-common:
    temperature: 0.3
    max_tokens: 1024
    top_p: 0.7

experiments:
  - name: "claude3-haiku"
    ep_name: "anthropic.claude-3-haiku-20240307-v1:0"
    model_id: "anthropic.claude-3-haiku-20240307-v1:0"
    inference_script: bedrock_predictor.py
    instance_type: "bedrock-model"
    instance_count: 1
    deploy: false
    inference_spec:
      parameter_set: bedrock-common
      stream: false

  - name: "claude3-sonnet"
    ep_name: "anthropic.claude-3-sonnet-20240229-v1:0"
    model_id: "anthropic.claude-3-sonnet-20240229-v1:0"
    inference_script: bedrock_predictor.py
    instance_type: "bedrock-model"
    instance_count: 1
    deploy: false
    inference_spec:
      parameter_set: bedrock-common
      stream: false

  - name: "llama3-70b"
    ep_name: "meta.llama3-1-70b-instruct-v1:0"
    model_id: "meta.llama3-1-70b-instruct-v1:0"
    inference_script: bedrock_predictor.py
    instance_type: "bedrock-model"
    instance_count: 1
    deploy: false
    inference_spec:
      parameter_set: bedrock-common
      stream: false
      
concurrency_levels:
  - 1
  - 2
  - 4
payload_files:
  - payload-en-0-500.jsonl
  - payload-en-501-1000.jsonl
  - payload-en-1001-1500.jsonl

pricing: "pricing.yml"

report:
  all_metrics_file: all_metrics-{datetime}.csv
  per_inference_request_file: per_inference_request-{datetime}.csv