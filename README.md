<h1 align="center">
        <img src="https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/img/fmbt-small.png?raw=true" width="25"></img> FMBench
</h1>

<p align="center">
    <p align="center">Benchmark any Foundation Model - Run Locally
    <br>
</p>
<h4 align="center"><a href="" target="_blank">OpenAI</a> | <a href="" target="_blank">Azure</a> | <a href="" target="_blank">Google</a> | <a href="" target="_blank">Anthropic</a></h4>
<h4 align="center">
    <a href="https://pypi.org/project/fmbench/" target="_blank">
        <img src="https://img.shields.io/pypi/v/fmbench.svg" alt="PyPI Version">
    </a>    
</h4>


üö® **What's new**: Simplified local execution - benchmark any model from your machine without complex infrastructure. üö®

`FMBench` is a Python package for running performance benchmarks and accuracy tests for **any Foundation Model**. The tool runs completely locally and supports multiple model providers including OpenAI, Azure OpenAI, Google, and Anthropic.

## Key Features

1. **Simple Local Setup**: Run the benchmarking tool directly on your machine, without complex infrastructure.

2. **Multi-Provider Support**: Benchmark models from OpenAI, Azure OpenAI, Google, and Anthropic with unified metrics.

3. **Comprehensive Metrics**: Measure performance (latency, throughput, tokens/second) and accuracy.

4. **Model Comparison**: Compare multiple models side-by-side to find the best one for your use case.

## Quickstart

```bash
# Install FMBench
pip install -U fmbench

# Set up local environment
mkdir -p ~/fmbench/{configs,results}
export FMBENCH_LOCAL_MODE=yes
export FMBENCH_RESULTS_DIR=~/fmbench/results

# Create a configuration file for OpenAI models
cat > ~/fmbench/configs/config.yml << EOF
general:
  name: openai-benchmark
aws:
  region: us-east-1
  s3_and_or_local_file_system: local
experiments:
  - name: gpt-4
    model_id: gpt-4
    model_name: GPT-4
    ep_name: gpt-4
    instance_type: gpt-4
    deploy: no
    inference_script: litellm_predictor.py
    inference_spec:
      parameter_set: openai
inference_parameters:
  openai:
    temperature: 0.1
    max_tokens: 100
    api_keys:
      OPENAI_API_KEY: "your-api-key-here"
EOF

# Run the benchmark
fmbench --config-file ~/fmbench/configs/config.yml --local-mode yes
```

## Supported Models

| Provider                      | Models                                     |
|:------------------------------|:-------------------------------------------|
| **OpenAI**                    | GPT-4, GPT-4o, GPT-3.5 Turbo              |
| **Azure OpenAI**              | Azure GPT-4, Azure GPT-4o, Azure GPT-3.5   |
| **Google**                    | Gemini Pro, Gemini 1.5 Pro, Gemini 1.5 Flash|
| **Anthropic**                 | Claude 3 Opus, Claude 3 Sonnet, Claude 3 Haiku |

## What's New

### 2.1.6

1. **Simplified Local Mode**: Run benchmarks directly on your machine with minimal setup
2. **External Model Support**: Benchmark OpenAI, Azure OpenAI, Google, and Anthropic models
3. **No Infrastructure Required**: No need for AWS accounts, SageMaker, EC2, or S3 buckets
4. **Unified Metrics**: Compare performance across multiple providers with consistent metrics

[See full release history](./release_history.md)

## Getting Started

`FMBench` is available as a Python package on [PyPi](https://pypi.org/project/fmbench) and is run as a command line tool once it is installed. 

> [!IMPORTANT]
> üí° [All **documentation** for `FMBench` is available on the `FMBench` website](https://aws-samples.github.io/foundation-model-benchmarking-tool/index.html)  

You can run `FMBench` on your local machine with minimal setup. See our [Quickstart Guide](https://aws-samples.github.io/foundation-model-benchmarking-tool/quickstart.html) for detailed instructions.

### Simple Three-Step Setup

1. **Install FMBench**:
   ```bash
   pip install -U fmbench
   ```

2. **Create a configuration file**:
   ```bash
   mkdir -p ~/fmbench/configs
   # Create config.yml with your model settings
   ```

3. **Run your benchmark**:
   ```bash
   export FMBENCH_LOCAL_MODE=yes
   fmbench --config-file ~/fmbench/configs/config.yml --local-mode yes
   ```

## Results

After running a benchmark, you'll find detailed results in your specified output directory, including:

- Performance metrics (latency, throughput, token rates)
- Model accuracy evaluations
- Comparative analysis across models
- CSV and JSON data for further analysis

Here is a screenshot of the report generated by `FMBench`:
![Report](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/img/results.gif?raw=true)

## Documentation

For detailed instructions, see:
- [Quickstart Guide](https://aws-samples.github.io/foundation-model-benchmarking-tool/quickstart.html)
- [Local Mode Documentation](https://aws-samples.github.io/foundation-model-benchmarking-tool/local_mode.html)
- [External Models Guide](https://aws-samples.github.io/foundation-model-benchmarking-tool/external_models.md)

## Enhancements

View the [ISSUES](https://github.com/aws-samples/foundation-model-benchmarking-tool/issues) on GitHub and add any you might think be an beneficial iteration to this benchmarking harness.

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This library is licensed under the MIT-0 License. See the [LICENSE](./LICENSE) file.

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=aws-samples/foundation-model-benchmarking-tool&type=Date)](https://star-history.com/#aws-samples/foundation-model-benchmarking-tool&Date)

[![Stargazers repo roster for @aws-samples/foundation-model-benchmarking-tool](https://reporoster.com/stars/aws-samples/foundation-model-benchmarking-tool)](https://github.com/aws-samples/foundation-model-benchmarking-tool/stargazers)

## Support

- Schedule Demo üëã - send us an email üôÇ
- [Community Discord üí≠](https://discord.gg/ydXV8mYFtF)
- Our emails ‚úâÔ∏è aroraai@amazon.com / madhurpt@amazon.com


## Contributors

<a href="https://github.com/aws-samples/foundation-model-benchmarking-tool/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=aws-samples/foundation-model-benchmarking-tool" />
</a>

## References
<a id="1">[1]</a> 
[Pat Verga et al., "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models",    arXiv:2404.18796, 2024.](https://arxiv.org/abs/2404.18796)